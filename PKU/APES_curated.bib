@article{Akaike1973,
author = {Akaike, Hirotugu},
isbn = {0018-9286 VO - 19},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
pmid = {1100705},
title = {A New Look at the Statistical Model Identification},
volume = {19},
number = {6},
year = {1973}
}
@article{Audibert2011,
	archivePrefix = {arXiv},
	arxivId = {1010.0074},
	author = {Audibert, Jean-Yves and Catoni, Olivier},
	issn = {0090-5364},
	journal = {Annals of Statistics},
	number = {5},
	pages = {2766--2794},
	title = {Robust linear least squares regression},
	volume = {39},
	year = {2011}
}
@article{Bertsimas2017,
	author = {Bertsimas, Dimitris and King, Angela},
	journal = {The Institute of Mathematical Statistics},
	title = {Logistic Regression : From Art to Science},
	volume = {32},
	year = {2017},
	month = {08},
	number = {3},
	pages = {367--384},
}
@article{Bertsimas2016,
	abstract = {In the period 1991–2015, algorithmic advances in Mixed Integer Optimization (MIO) coupled with hardware improvements have resulted in an astonishing 450 billion factor speedup in solving MIO problems. We present a MIO approach for solving the classical best subset selection problem of choosing kk out of pp features in linear regression given nn observations. We develop a discrete extension of modern first-order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with nn in the 1000s and pp in the 100s in minutes to provable optimality, and finds near optimal solutions for nn in the 100s and pp in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than Lasso and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.},
	author = {Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
	file = {:Users/kevinwang/Documents/Mendeley Desktop/Bertsimas, King, Mazumder/Annals of Statistics/Bertsimas, King, Mazumder - 2016 - Best subset selection via a modern optimization lens.pdf:pdf},
	isbn = {0001415123},
	issn = {00905364},
	journal = {Annals of Statistics},
	number = {2},
	pages = {813--852},
	title = {Best subset selection via a modern optimization lens},
	volume = {44},
	year = {2016}
}
@article{Cheng2018,
	author = {Cheng, Ming-Yen and Feng, Sanying and Li, Gaorong and Lian, Heng},
	issn = {1369-1473},
	journal = {Australian {\&} New Zealand Journal of Statistics},
	keywords = {big data problems; high-dimensional statistical in},
	number = {1, SI},
	pages = {20--42},
	title = {Greedy forward regression for variable screening},
	volume = {60},
	year = {2018}
}
@book{Claeskens2008,
	author = {Claeskens, Gerda and Hjort, Lid Nils},
	isbn = {9780521852258},
	title = {Model Selection and Model Averaging},
	year = {2008},
	series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
	title={Model Selection and Model Averaging},
	publisher={Cambridge University Press, Cambridge}, 
	collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}
@article{Firth1993,
	author = {Firth},
	file = {:Users/kevinwang/Documents/Mendeley Desktop/Firth/Unknown/Firth - 1993 - Bias Reduction of Maximum Likelihood Estimates.pdf:pdf},
	journal = {Biometrika},
	number = {1},
	pages = {27--38},
	title = {Bias Reduction of Maximum Likelihood Estimates},
	volume = {80},
	year = {1993}
}
@article{Friedman2010,
	author = {Jerome Friedman and Trevor Hastie and Rob Tibshirani},
	title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
	journal = {Journal of Statistical Software},
	volume = {33},
	number = {1},
	year = {2010},
	pages = {1--22}
}
@article{Furnival1974,
	author = {Furnival, G and Wilson, R},
	file = {:Users/kevinwang/Documents/Mendeley Desktop/Furnival, Wilson/Technometrics/Furnival, Wilson - 1974 - Regressions by Leaps and Bounds.pdf:pdf},
	journal = {Technometrics},
	number = {4},
	pages = {499--511},
	title = {Regressions by Leaps and Bounds},
	volume = {16},
	year = {1974}
}
@article{Hastie2017,
	archivePrefix = {arXiv},
	arxivId = {1707.08692},
	author = {Hastie, Trevor and Tibshirani, Robert and Tibshirani, Ryan J},
	title = {Extended Comparisons of Best Subset Selection, Forward Stepwise Selection, and the {L}asso Following `` {B}est Subset Selection from a Modern Optimization Lens " by {B}ertsimas, {K}ing, and {M}azumder (2016)},
	journal = {ArXiv e-prints},
	eprint = {1707.08692},
	year = {2017}
}
@book{Hastie2015,
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	isbn = {978-1-4987-1217-0},
	issn = {0306-7734},
	publisher = {Chapman \& Hall/CRC, ‎New York},
	title = {Statistical Learning with Sparsity: The {L}asso and Generalizations},
	year = {2015}
}
@article{Hazimeh2018,
	author = {Hazimeh, Hussein and Mazumder, Rahul},
	title = {Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms},
	journal = {ArXiv e-prints},
	eprint = {1803.01454},
	year = {2018}
}
@article{Hosmer1989,
	author = {David W. Hosmer and Borko Jovanovic and Stanley Lemeshow},
	journal = {Biometrics},
	number = {4},
	pages = {1265--1270},
	title = {Best Subsets Logistic Regression},
	volume = {45},
	year = {1989}
}
@Manual{Lumley2017,
	title = {leaps: Regression Subset Selection},
	author = {Thomas Lumley and Alan Miller},
	year = {2017},
	note = {R package version 3.0},
	url = {https://CRAN.R-project.org/package=leaps},
}
@book{Mccullagh1989,
	author = {McCullagh, P. and Nelder, J. A.},
	isbn = { 9780412317606},
	title = {Generalized Linear Models},
	year = {1989},
	publisher = {Chapman \& Hall / CRC, New York}
}
@Manual{Mcleod2010,
	title = {bestglm: Best Subset GLM and Regression Utilities},
	author = {A.I. McLeod and Changjiang Xu},
	year = {2018},
	note = {R package version 0.37},
	url = {https://CRAN.R-project.org/package=bestglm},
}
@article{Molania,
	author = {Molania, Ramyar and Gagnon-Bartsch, Johann A and Dobrovic, Alexander and Speed, Terence P},
	title = {A new normalization for the {NanoString} {nCounter} gene expression assay},
	year = {2018},
	journal = {bioRxiv preprint}
}
@article{Muller2005,
	author = {M{\"{u}}ller, Samuel and Welsh, a. H},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	number = {472},
	pages = {1297--1310},
	title = {Outlier Robust Model Selection in Linear Regression},
	volume = {100},
	year = {2005}
}

@article{Muller2010,
	author = {M{\"{u}}ller, Samuel and Welsh, Alan H.},
	journal = {International Statistical Review},
	keywords = {Akaike information criterion (AIC),Bayesian information criterion (BIC),Generalized information criterion (GIC),Linear regression,Model selection,Model selection curves},
	number = {2},
	pages = {240--256},
	title = {On model selection curves},
	volume = {78},
	year = {2010}
}
@article{Murray2013,
	author = {Murray, K and Heritier, S and M{\"{u}}ller, S},
	keywords = {akaike information criterion,bayesian information,criterion,generalized linear models,graphical methods,model selection,model selection curves,variable selection},
	title = {Graphical tools for model selection in generalized linear models},
	journal = {Statistics in Medicine},
	year = {2013},
	volume = {32},
	pages = {4438-4451}
}
@article{Nelder1972,
	author = {Nelder, J A and Wedderburn, R W M},
	issn = {00359238},
	journal = {Journal of the Royal Statistical Society. Series A},
	number = {3},
	pages = {370--384},
	title = {Generalized Linear Models},
	volume = {135},
	year = {1972}
}
@article{Peloquin2016,
author = {Joanna M. Peloquin AND Gautam Goel AND Lingjia Kong AND Hailiang Huang AND Talin Haritunians AND R. Balfour Sartor AND Mark J. Daly AND Rodney D. Newberry AND Dermot P. McGovern AND Vijay Yajnik AND Sergio A. Lira AND Ramnik J. Xavier},
journal = {Journal of Clinical Investigation Insight},
publisher = {The American Society for Clinical Investigation},
title = {Characterization of candidate genes in inflammatory bowel disease associated risk loci},
year = {2016},
month = {8},
volume = {1},
number = {13}
}
@article{Schwarz1978,
	author = {Schwarz, Gideon},
	isbn = {0780394224},
	issn = {0090-5364},
	journal = {Annals of Statistics},
	number = {2},
	pages = {461--464},
	pmid = {2958889},
	title = {Estimating the Dimension of a Model},
	volume = {6},
	year = {1978}
}
@article{Shao1997,
	author = {Shao, Jun},
	file = {:Users/kevinwang/Documents/Mendeley Desktop/Shao/Statistica Sinica/Shao - 1997 - An asymptotic theory for linear model selection.pdf:pdf},
	issn = {10170405},
	journal = {Statistica Sinica},
	keywords = {aic,and phrases,asymptotic loss efficiency,bic,c p,consistency,cross-,gic,squared error loss,validation},
	pages = {221--264},
	title = {An asymptotic theory for linear model selection},
	volume = {7},
	year = {1997}
}
@article{Su2017,
	author = {Su, Weijie and Bogdan, Malgorzata and Cand{\`{e}}s, Emmanuel},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Adaptive selection of parameters,Approximate message passing (AMP),False discovery rate,False negative rate,Lasso,Lasso path,Power,Su2017a},
	mendeley-tags = {Su2017a},
	number = {5},
	pages = {2133--2150},
	title = {False discoveries occur early on the {L}asso path},
	volume = {45},
	year = {2017}
}
@article{Su2018,
	author = {Su, Weijie J},
	title = {When is the first spurious variable selected by sequential regression procedures?},
	journal = {Biometrika},
	volume = {105},
	number = {3},
	pages = {517-527},
	year = {2018}
}
@article{Tarr2018,
	author = {Tarr, Garth and M{\"{u}}ller, Samuel and Welsh, Alan H},
	journal = {Journal of Statistical Software},
	number = {9},
	pages = {1--28},
	title = {mplot: An {R} Package for Graphical Model Stability and Variable Selection Procedures},
	volume = {83},
	year = {2018}
}
@article{Tibshirani1996,
	author = {Tibshirani, Robert},
	journal = {Journal of the Royal Statistical Society B},
	isbn = {0849320240},
	issn = {00359246},
	number = {1},
	pages = {267--288},
	title = {Regression Selection and Shrinkage via the {L}asso},
	volume = {58},
	year = {1996}
}
@article{Yang2005,
	author = {Yang, Y},
	journal = {Biometrika},
	keywords = {Model selection Modelling Regression Linear model},
	number = {April},
	pages = {937--950},
	title = {Can the strengths of {AIC} and {BIC} be shared? A conflict between model identification and regression estimation},
	volume = {92},
	year = {2005}
}
@article{Zhao2006,
	author = {Zhao, Peng and Yu, Bin},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {The Journal of Machine Learning Research},
	keywords = {consistency,lasso,model selection,regularization,sparsity},
	pages = {2541--2563},
	title = {On Model Selection Consistency of {L}asso},
	volume = {7},
	year = {2006}
}
@article{Efron2004,
	author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
	journal = {Annals of Statistics},
	pages = {407--499},
	publisher = {The Institute of Mathematical Statistics},
	title = {Least angle regression},
	volume = {32},
	year = {2004}
}

@book{Burnham2002,
	author = {Burnham, K.P. and Anderson, D.R.},
	pmid = {48557578},
	title = {Model Selection and Multimodel Inference},
	year = {2002},
	publisher = {Springer, New York}
}

@article{Hastie2005,
	author = {Hastie, Trevor and Zou, Hui},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {2},
	pages = {301--320},
	title = {{Regularization and Variable Selection via the Elastic Net}},
	volume = {67},
	year = {2005}
}

@book{HosmerBook1989,
	author = {Hosmer, David W. and Lemeshow, Stanley},
	title = {{Applied Logistic Regression}},
	publisher = {Wiley, New York},
	year = {1989}
}
@book{Venables2002,
	author = {Venables, W. N. and Ripley, B. D.},
	title = {{Modern Applied Statistics with S}},
	year = {2002},
	publisher = {Springer, New York}
}
@article{Heinze2018,
	author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
	journal = {Biometrical Journal},
	number = {3},
	pages = {431--449},
	title = {{Variable selection – A review and recommendations for the practicing statistician}},
	volume = {60},
	year = {2018}
}
@article{Claeskens2016,
	author = {Claeskens, Gerda},
	journal = {Annual Review of Statistics and Its Application},
	volume = {3},
	pages = {233--258},
	title = {{Statistical Model Choice}},
	year = {2016}
}
@article{Tibshirani2010,
	author = {Tibshirani, Robert and Hastie, Trevor and Bien, Jacob and Simon, Noah and Friedman, Jerome H and Taylor, Jonathan and Tibshirani, Ryan},
	journal = {Journal of Royal Statistical Society. Series B},
	number = {2},
	pages = {245--266},
	title = {{Strong Rules for Discarding Predictors in Lasso-type Problems}},
	volume = {74},
	year = {2010}
}

@article{Clyde2011,
	author = {Clyde, Merlise A. and Ghosh, Joyee and Littman, Michael L.},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {80--101},
	title = {{Bayesian adaptive sampling for variable selection and model averaging}},
	volume = {20},
	year = {2011}
}

@article{Mallows1973,
	author = {Mallows, C L},
	journal = {Technometrics},
	number = {4},
	pages = {661--675},
	title = {{Some Comments on Cp}},
	volume = {15},
	year = {1973}
}

@article{Muller2013,
	author = {M{\"{u}}ller, Samuel and Scealy, J. L. and Welsh, A. H.},
	issn = {0883-4237},
	journal = {Statistical Science},
	number = {2},
	pages = {135--167},
	title = {{Model Selection in Linear Mixed Models}},
	volume = {28},
	year = {2013}
}

