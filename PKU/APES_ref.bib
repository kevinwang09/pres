@article{Bertsimas2017,
author = {Bertsimas, Dimitris and King, Angela},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Statistical Science/Bertsimas, King - 2017 - Logistic regression From art to science.pdf:pdf},
journal = {Statistical Science},
keywords = {and phrases,computational statistics,logistic regression,mixed integer nonlinear optimization},
number = {3},
pages = {367--384},
title = {{Logistic regression: From art to science}},
volume = {32},
year = {2017}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.2307/2346178},
eprint = {11/73273},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of the Royal Statistical Society Series B/Tibshirani - 1996 - Regression shrinkage and selection via the Lasso.pdf:pdf},
isbn = {0849320240},
issn = {00359246},
journal = {Journal of the Royal Statistical Society Series B},
number = {152},
pages = {10--14},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression shrinkage and selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.18637/jss.v033.i01},
eprint = {NIHMS150003},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of Statistical Software/Friedman, Hastie, Tibshirani - 2010 - Regularization paths for generalized linear models via coordinate descent.pdf:pdf},
isbn = {8015815967},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Coordinate-descent,Elastic net,L 1 penalty,Lasso,Logistic regression,Regularization path},
number = {1},
pages = {1--22},
pmid = {20808728},
title = {{Regularization paths for generalized linear models via coordinate descent}},
volume = {33},
year = {2010}
}
@article{Tarr2018,
abstract = {The mplot package provides an easy to use implementation of model stability and variable inclusion plots (M$\backslash$"uller and Welsh 2010; Murray, Heritier, and M$\backslash$"uller 2013) as well as the adaptive fence (Jiang, Rao, Gu, and Nguyen 2008; Jiang, Nguyen, and Rao 2009) for linear and generalised linear models. We provide a number of innovations on the standard procedures and address many practical implementation issues including the addition of redundant variables, interactive visualisations and approximating logistic models with linear models. An option is provided that combines our bootstrap approach with glmnet for higher dimensional models. The plots and graphical user interface leverage state of the art web technologies to facilitate interaction with the results. The speed of implementation comes from the leaps package and cross-platform multicore support.},
author = {Tarr, Garth and M{\"{u}}ller, Samuel and Welsh, Alan H.},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of Statistical Software/Tarr, M{\"{u}}ller, Welsh - 2018 - mplot An R package for graphical model stability and variable selection procedures.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {Fence,Generalized linear models,Linear models,Mixed models,Model selection,R,Variable selection},
number = {9},
pages = {1--28},
title = {{mplot: An R package for graphical model stability and variable selection procedures}},
volume = {83},
year = {2018}
}
@article{Peloquin2016,
abstract = {GWAS have linked SNPs to risk of inflammatory bowel disease (IBD), but a systematic characterization of disease-associated genes has been lacking. Prior studies utilized microarrays that did not capture many genes encoded within risk loci or defined expression quantitative trait loci (eQTLs) using peripheral blood, which is not the target tissue in IBD. To address these gaps, we sought to characterize the expression of IBD-associated risk genes in disease-relevant tissues and in the setting of active IBD. Terminal ileal (TI) and colonic mucosal tissues were obtained from patients with Crohn's disease or ulcerative colitis and from healthy controls. We developed a NanoString code set to profile 678 genes within IBD risk loci. A subset of patients and controls were genotyped for IBD-associated risk SNPs. Analyses included differential expression and variance analysis, weighted gene coexpression network analysis, and eQTL analysis. We identified 116 genes that discriminate between healthy TI and colon samples and uncovered patterns in variance of gene expression that highlight heterogeneity of disease. We identified 107 coexpressed gene pairs for which transcriptional regulation is either conserved or reversed in an inflammation-independent or -dependent manner. We demonstrate that on average approximately 60{\%} of disease-associated genes are differentially expressed in inflamed tissue. Last, we identified eQTLs with either genotype-only effects on expression or an interaction effect between genotype and inflammation. Our data reinforce tissue specificity of expression in disease-associated candidate genes, highlight genes and gene pairs that are regulated in disease-relevant tissue and inflammation, and provide a foundation to advance the understanding of IBD pathogenesis.},
author = {Peloquin, Joanna M. and Goel, Gautam and Kong, Lingjia and Huang, Hailiang and Haritunians, Talin and Sartor, R. Balfour and Daly, Mark J. and Newberry, Rodney D. and McGovern, Dermot P. and Yajnik, Vijay and Lira, Sergio A. and Xavier, Ramnik J.},
doi = {10.1172/jci.insight.87899},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of Clinical Investigation Insight/Peloquin et al. - 2016 - Characterization of candidate genes in inflammatory bowel disease - associated risk loci.pdf:pdf},
issn = {2379-3708},
journal = {Journal of Clinical Investigation Insight},
number = {13},
pages = {e87899},
pmid = {27668286},
publisher = {The American Society for Clinical Investigation},
title = {{Characterization of candidate genes in inflammatory bowel disease - associated risk loci}},
volume = {1},
year = {2016}
}
@article{Su2017,
abstract = {In regression settings where explanatory variables have very low correlations and there are relatively few effects, each of large magnitude, we expect the Lasso to find the important variables with few errors, if any. This paper shows that in a regime of linear sparsity---meaning that the fraction of variables with a non-vanishing effect tends to a constant, however small---this cannot really be the case, even when the design variables are stochastically independent. We demonstrate that true features and null features are always interspersed on the Lasso path, and that this phenomenon occurs no matter how strong the effect sizes are. We derive a sharp asymptotic trade-off between false and true positive rates or, equivalently, between measures of type I and type II errors along the Lasso path. This trade-off states that if we ever want to achieve a type II error (false negative rate) under a critical value, then anywhere on the Lasso path the type I error (false positive rate) will need to exceed a given threshold so that we can never have both errors at a low level at the same time. Our analysis uses tools from approximate message passing (AMP) theory as well as novel elements to deal with a possibly adaptive selection of the Lasso regularizing parameter.},
archivePrefix = {arXiv},
arxivId = {1511.01957},
author = {Su, Weijie and Bogdan, Malgorzata and Cand{\`{e}}s, Emmanuel},
doi = {10.1214/16-AOS1521},
eprint = {1511.01957},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Annals of Statistics/Su, Bogdan, Cand{\`{e}}s - 2017 - False discoveries occur early on the lasso path.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Adaptive selection of parameters,Approximate message passing (AMP),False discovery rate,False negative rate,Lasso,Lasso path,Power,Su2017a},
mendeley-tags = {Su2017a},
number = {5},
pages = {2133--2150},
title = {{False discoveries occur early on the lasso path}},
volume = {45},
year = {2017}
}
@article{Su2018,
abstract = {Applied statisticians use sequential regression procedures to produce a ranking of explanatory variables and, in settings of low correlations between variables and strong true effect sizes, expect that variables at the very top of this ranking are truly relevant to the response. In a regime of certain sparsity levels, however, three examples of sequential procedures--forward stepwise, the lasso, and least angle regression--are shown to include the first spurious variable unexpectedly early. We derive a rigorous, sharp prediction of the rank of the first spurious variable for these three procedures, demonstrating that the first spurious variable occurs earlier and earlier as the regression coefficients become denser. This counterintuitive phenomenon persists for statistically independent Gaussian random designs and an arbitrarily large magnitude of the true effects. We gain a better understanding of the phenomenon by identifying the underlying cause and then leverage the insights to introduce a simple visualization tool termed the double-ranking diagram to improve on sequential methods. As a byproduct of these findings, we obtain the first provable result certifying the exact equivalence between the lasso and least angle regression in the early stages of solution paths beyond orthogonal designs. This equivalence can seamlessly carry over many important model selection results concerning the lasso to least angle regression.},
archivePrefix = {arXiv},
arxivId = {1708.03046},
author = {Su, Weijie J.},
doi = {10.1093/biomet/asy032},
eprint = {1708.03046},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Biometrika/Su - 2018 - When is the first spurious variable selected by sequential regression procedures.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {False variable,Familywise error rate,Forward stepwise regression,Lasso,Least angle regression,Su2018},
mendeley-tags = {Su2018},
number = {3},
pages = {517--527},
title = {{When is the first spurious variable selected by sequential regression procedures?}},
volume = {105},
year = {2018}
}
@article{Tibshirani2010,
author = {Tibshirani, Robert Ryan and Hastie, Trevor and Bien, Jacob and Simon, Noah and Friedman, Jerome H and Taylor, Jonathan and Tibshirani, Robert Ryan},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of Royal Statistical Society Series B/Tibshirani et al. - 2010 - Strong rules for discarding predictors in Lasso-type problems.pdf:pdf},
journal = {Journal of Royal Statistical Society Series B},
keywords = {convex optimization,l 1 -regularization,lasso,screening,sparsity},
number = {2},
pages = {245--266},
title = {{Strong rules for discarding predictors in Lasso-type problems}},
volume = {74},
year = {2010}
}
@article{Zhao2006,
abstract = {Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are "irrepresentable" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.},
archivePrefix = {arXiv},
arxivId = {1305.7477},
author = {Zhao, Peng and Yu, Bin},
doi = {10.1109/TIT.2006.883611},
eprint = {1305.7477},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/The Journal of Machine Learning Research/Zhao, Yu - 2006 - On model selection consistency of Lasso.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {consistency,lasso,model selection,regularization,sparsity},
number = {Nov},
pages = {2541--2563},
title = {{On model selection consistency of Lasso}},
volume = {7},
year = {2006}
}
@article{Nelder1972,
abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
author = {Nelder, J A and Wedderburn, R W M},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Unknown/Schindler - 2010 - Evolution of Phosphorus Limitation in Lakes Author ( s ) D . W . Schindler Published by American Association for the.pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society Series A},
number = {3},
pages = {370--384},
publisher = {[Royal Statistical Society, Wiley]},
title = {{Generalized linear models}},
volume = {135},
year = {1972}
}
@article{Claeskens2016,
abstract = {Variable selection methods and model selection approaches are valuable statistical tools that are indispensable for almost any statistical modeling question. This review first considers the use of information criteria for model selection. Such criteria provide an ordering of the considered models where the best model is selected. Different modeling goals might require different criteria to be used. Next, the effect of including a penalty in the estimation process is discussed. Third, nonparametric estimation is discussed; it contains several aspects of model choice, such as the choice of the estimator to use and the selection of tuning parameters. Fourth, model averaging approaches are reviewed in which estimators from different models are weighted to provide one final estimator. There are several ways to choose the weights, and most of them result in data-driven, hence random, weights. Challenges for inference after model selection and inference for model-averaged estimators are discussed.},
author = {Claeskens, Gerda},
doi = {10.1146/annurev-statistics-041715-033413},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Annual Review of Statistics and Its Application/Claeskens - 2016 - Statistical model choice(2).pdf:pdf},
isbn = {0417150334},
journal = {Annual Review of Statistics and Its Application},
number = {1},
pages = {233--258},
title = {{Statistical model choice}},
volume = {3},
year = {2016}
}
@article{Cheng2018,
abstract = {In the ultra-high dimensional setting, two popular variable screening methods with the desirable sure screening property are sure independence screening (SIS) and forward regression (FR). Both are classical variable screening methods, and recently have attracted greater attention under high-dimensional data analysis. We consider a new and simple screening method that incorporates multiple predictors at each step of forward regression, with decisions on which variables to incorporate based on the same criterion. If only one step is carried out, the new procedure reduces to SIS. Thus it can be regarded as a generalisation and unification of FR and SIS. More importantly, it preserves the sure screening property and has computational complexity similar to FR at each step, yet it can discover the relevant covariates in fewer steps. Thus it reduces the computational burden of FR drastically while retaining the advantages of the latter over SIS. Furthermore, we show that it can find all the true variables if the number of steps taken is the same as the correct model size, which is a new theoretical result even for the original FR. An extensive simulation study and application to two real data examples demonstrate excellent performance of the proposed method.},
author = {Cheng, MingYen and Feng, Sanying and Li, Gaorong and Lian, Heng},
doi = {10.1111/anzs.12218},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Australian {\&} New Zealand Journal of Statistics/Cheng et al. - 2018 - Greedy forward regression for variable screening.pdf:pdf},
issn = {1369-1473},
journal = {Australian {\&} New Zealand Journal of Statistics},
keywords = {big data problems,high-dimensional statistical in},
number = {1},
pages = {20--42},
title = {{Greedy forward regression for variable screening}},
volume = {60},
year = {2018}
}
@article{Audibert2011,
abstract = {We consider the problem of robustly predicting as well as the best linear combination of {\$}d{\$} given functions in least squares regression, and variants of this problem including constraints on the parameters of the linear combination. For the ridge estimator and the ordinary least squares estimator, and their variants, we provide new risk bounds of order {\$}d/n{\$} without logarithmic factor unlike some standard results, where {\$}n{\$} is the size of the training data. We also provide a new estimator with better deviations in the presence of heavy-tailed noise. It is based on truncating differences of losses in a min--max framework and satisfies a {\$}d/n{\$} risk bound both in expectation and in deviations. The key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations. All risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. Experimental results strongly back up our truncated min--max estimator.},
archivePrefix = {arXiv},
arxivId = {1010.0074},
author = {Audibert, JeanYves and Catoni, Olivier},
doi = {10.1214/11-AOS918},
eprint = {1010.0074},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Annals of Statistics/Audibert, Catoni - 2011 - Robust linear least squares regression.pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {62J05,62J07,Linear regression,and phrases,article published by the,bayesian theorems,distributions,generalization er,generalization error,gibbs posterior,linear regression,pac-,randomized estimators,reprint of the original,resistant estimators,risk bounds,robust statistics,shrinkage,statistical learning theory,this is an electronic},
number = {5},
pages = {2766--2794},
title = {{Robust linear least squares regression}},
volume = {39},
year = {2011}
}
@article{Akaike1973,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
eprint = {arXiv:1011.1669v3},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/IEEE Transactions on Automatic Control/Akaike - 1973 - A new look at the statistical model identification.pdf:pdf},
isbn = {0018-9286 VO - 19},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
pmid = {1100705},
title = {{A new look at the statistical model identification}},
volume = {19},
year = {1973}
}
@article{Hosmer1989,
author = {Hosmer, David W and Jovanovic, Borko and Lemeshow, Stanley},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Biometrics/Hosmer, Jovanovic, Lemeshow - 1989 - Best subsets logistic regression.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {1265--1270},
title = {{Best subsets logistic regression}},
volume = {45},
year = {1989}
}
@article{Heinze2018,
abstract = {Statistical models support medical research by facilitating individualized outcome prognostication conditional on independent variables or by estimating effects of risk factors adjusted for covariates. Theory of statistical models is well-established if the set of independent variables to consider is fixed and small. Hence, we can assume that effect estimates are unbiased and the usual methods for confidence interval estimation are valid. In routine work, however, it is not known a priori which covariates should be included in a model, and often we are confronted with the number of candidate variables in the range 10–30. This number is often too large to be considered in a statistical model. We provide an overview of various available variable selection methods that are based on significance or information criteria, penalized likelihood, the change-in-estimate criterion, background knowledge, or combinations thereof. These methods were usually developed in the context of a linear regression model and then transferred to more generalized linear models or models for censored survival data. Variable selection, in particular if used in explanatory modeling where effect estimates are of central interest, can compromise stability of a final model, unbiasedness of regression coefficients, and validity of p-values or confidence intervals. Therefore, we give pragmatic recommendations for the practicing statistician on application of variable selection methods in general (low-dimensional) modeling problems and on performing stability investigations and inference. We also propose some quantities based on resampling the entire variable selection process to be routinely reported by software packages offering automated variable selection algorithms},
author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
doi = {10.1002/bimj.201700067},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Biometrical Journal/Heinze, Wallisch, Dunkler - 2018 - Variable selection - A review and recommendations for the practicing statistician.pdf:pdf},
issn = {15214036},
journal = {Biometrical Journal},
keywords = {change-in-estimate criterion,penalized likelihood,resampling,statistical model,stepwise selection},
number = {3},
pages = {431--449},
title = {{Variable selection - A review and recommendations for the practicing statistician}},
volume = {60},
year = {2018}
}
@article{Hazimeh2018,
abstract = {We consider the canonical {\$}L{\_}0{\$}-regularized least squares problem (aka best subsets) which is generally perceived as a `gold-standard' for many sparse learning regimes. In spite of worst-case computational intractability results, recent work has shown that advances in mixed integer optimization can be used to obtain near-optimal solutions to this problem for instances where the number of features {\$}p \backslashapprox 10{\^{}}3{\$}. While these methods lead to estimators with excellent statistical properties, often there is a price to pay in terms of a steep increase in computation times, especially when compared to highly efficient popular algorithms for sparse learning (e.g., based on {\$}L{\_}1{\$}-regularization) that scale to much larger problem sizes. Bridging this gap is a main goal of this paper. We study the computational aspects of a family of {\$}L{\_}0{\$}-regularized least squares problems with additional convex penalties. We propose a hierarchy of necessary optimality conditions for these problems. We develop new algorithms, based on coordinate descent and local combinatorial optimization schemes, and study their convergence properties. We demonstrate that the choice of an algorithm determines the quality of solutions obtained; and local combinatorial optimization-based algorithms generally result in solutions of superior quality. We show empirically that our proposed framework is relatively fast for problem instances with {\$}p\backslashapprox 10{\^{}}6{\$} and works well, in terms of both optimization and statistical properties (e.g., prediction, estimation, and variable selection), compared to simpler heuristic algorithms. A version of our algorithm reaches up to a three-fold speedup (with {\$}p{\$} up to {\$}10{\^{}}6{\$}) when compared to state-of-the-art schemes for sparse learning such as glmnet and ncvreg.},
archivePrefix = {arXiv},
arxivId = {1803.01454},
author = {Hazimeh, Hussein and Mazumder, Rahul},
eprint = {1803.01454},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/arXiv e-prints/Hazimeh, Mazumder - 2018 - Fast best subset selection Coordinate descent and local combinatorial optimization algorithms.pdf:pdf},
isbn = {0001415123},
journal = {arXiv e-prints},
pages = {1--41},
title = {{Fast best subset selection: Coordinate descent and local combinatorial optimization algorithms}},
year = {2018}
}
@article{Hastie2017,
abstract = {In exciting new work, Bertsimas et al. (2016) showed that the classical best subset selection problem in regression modeling can be formulated as a mixed integer optimization (MIO) prob-lem. Using recent advances in MIO algorithms, they demonstrated that best subset selection can now be solved at much larger problem sizes that what was thought possible in the statistics community. They presented empirical comparisons of best subset selection with other popular variable selection procedures, in particular, the lasso and forward stepwise selection. Surpris-ingly (to us), their simulations suggested that best subset selection consistently outperformed both methods in terms of prediction accuracy. Here we present an expanded set of simulations to shed more light on these comparisons. The summary is roughly as follows: • neither best subset selection nor the lasso uniformly dominate the other, with best subset selection generally performing better in high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes; • best subset selection and forward stepwise perform quite similarly throughout; • the relaxed lasso (actually, a simplified version of the original relaxed estimator defined in Meinshausen, 2007) is the overall winner, performing just about as well as the lasso in low SNR scenarios, and as well as subset selection in high SNR scenarios. We emphasize that these results are based on prediction error as the metric of interest, and we note that different behaviors may occur for other metrics (e.g., a metric measuring the recovery of the correct variables in a sparse population linear model may yield different results).},
archivePrefix = {arXiv},
arxivId = {1707.08692},
author = {Hastie, Trevor and Tibshirani, Robert Ryan J and Tibshirani, Robert Ryan J},
eprint = {1707.08692},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/arXiv e-prints/Hastie, Tibshirani, Tibshirani - 2017 - Extended comparisons of best subset selection, forward stepwise selection, and the Lasso f(2016).pdf:pdf},
journal = {arXiv e-prints},
pages = {1--52},
title = {{Extended comparisons of best subset selection, forward stepwise selection, and the Lasso following ``Best subset selection from a modern optimization lens" by Bertsimas, King, and Mazumder (2016)}},
year = {2017}
}
@manual{Mcleod2010,
annote = {R package version 0.37},
author = {McLeod, A I and Xu, Changjiang},
title = {{bestglm: best subset GLM and regression utilities}},
url = {https://cran.r-project.org/package=bestglm},
year = {2018}
}
@article{Murray2013,
author = {Murray, K and Heritier, S and M{\"{u}}ller, S},
journal = {Statistics in Medicine},
keywords = {akaike information criterion,bayesian information,criterion,generalized linear models,graphical methods,model selection,model selection curves,variable selection},
number = {25},
pages = {4438--4451},
title = {{Graphical tools for model selection in generalized linear models}},
volume = {32},
year = {2013}
}
@article{Yang2005,
author = {Yang, Y},
journal = {Biometrika},
keywords = {Model selection Modelling Regression Linear model},
number = {4},
pages = {937--950},
title = {{Can the strengths of AIC and BIC be shared? A conflict between model identification and regression estimation}},
volume = {92},
year = {2005}
}
@book{Burnham2002,
author = {Burnham, K P and Anderson, D R},
pmid = {48557578},
publisher = {Springer, New York},
title = {{Model Selection and Multimodel Inference}},
year = {2002}
}
@article{Efron2004,
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
journal = {Annals of Statistics},
pages = {407--499},
publisher = {The Institute of Mathematical Statistics},
title = {{Least angle regression}},
volume = {32},
year = {2004}
}
@article{Hastie2005,
author = {Hastie, Trevor and Zou, Hui},
journal = {Journal of the Royal Statistical Society Series B},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
isbn = {978-1-4987-1217-0},
issn = {0306-7734},
publisher = {Chapman {\&} Hall/CRC, New York},
title = {{Statistical Learning with Sparsity: The Lasso and Generalizations}},
year = {2015}
}
@misc{Lumley2017,
annote = {R package version 3.0},
author = {Lumley, Thomas and Miller, Alan},
title = {leaps: regression subset selection},
url = {https://cran.r-project.org/package=leaps},
year = {2017}
}
@book{Claeskens2008,
author = {Claeskens, Gerda and Hjort, Lid Nils},
isbn = {9780521852258},
publisher = {Cambridge University Press, Cambridge},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Model Selection and Model Averaging}},
year = {2008}
}
@book{HosmerBook1989,
author = {Hosmer, David W and Lemeshow, Stanley},
publisher = {Wiley, New York},
title = {{Applied Logistic Regression}},
year = {1989}
}
@article{Yu2013,
author = {Yu, Bin},
doi = {10.3150/13-BEJSP14},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Bernoulli/Yu - 2013 - Stability.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {cross-validation,double exponential error,estimation stability,fmri,high-dim regression,lasso,movie reconstruction,robust statistics,stability},
number = {4},
pages = {1484--1500},
title = {{Stability}},
volume = {19},
year = {2013}
}
@inproceedings{Bach2008,
abstract = {We consider the least-square linear regression problem with regular-ization by the ℓ1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.},
archivePrefix = {arXiv},
arxivId = {0804.1302v1},
author = {Bach, Francis R},
booktitle = {International Conference on Machine Learning},
eprint = {0804.1302v1},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/International Conference on Machine Learning/Bach - 2008 - Bolasso model consistent Lasso estimation through the bootstrap.pdf:pdf},
title = {{Bolasso: model consistent Lasso estimation through the bootstrap}},
year = {2008}
}
@article{Molania2019,
abstract = {The Nanostring nCounter gene expression assay uses molecular barcodes and single molecule imaging to detect and count hundreds of unique transcripts in a single reaction. These counts need to be normalized to adjust for the amount of sample , variations in assay efficiency and other factors. Most users adopt the normalization approach described in the nSolver analysis software, which involves background correction based on the observed values of negative control probes, a within-sample normalization using the observed values of positive control probes and normalization across samples using reference (housekeeping) genes. Here we present a new normalization method, Removing Un-wanted Variation-III (RUV-III), which makes vital use of technical replicates and suitable control genes. We also propose an approach using pseudo-replicates when technical replicates are not available. The effectiveness of RUV-III is illustrated on four different datasets. We also offer suggestions on the design and analysis of studies involving this technology.},
author = {Molania, Ramyar and Gagnon-Bartsch, Johann A and Dobrovic, Alexander and Speed, Terence P},
doi = {10.1093/nar/gkz433},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Nucleic Acids Research/Molania et al. - 2019 - A new normalization for Nanostring nCounter gene expression data.pdf:pdf},
journal = {Nucleic Acids Research},
number = {12},
pages = {6073--6083},
title = {{A new normalization for Nanostring nCounter gene expression data}},
volume = {47},
year = {2019}
}
@article{Muller2005,
abstract = {We propose a new approach to the selection of regression models based on combining a robust penalized criterion and a robust conditional expected prediction loss function that is estimated using a stratified bootstrap. Both components of the procedure use robust criteria (i.e., robust $\rho$-functions) rather than squared error loss to reduce the effects of large residuals and poor bootstrap samples. A key idea is to separate estimation from model selection by choosing estimators separately from the $\rho$-function. Using the stratified bootstrap further reduces the likelihood of obtaining poor bootstrap samples. We show that the model selection procedure is consistent under some conditions and works well in our simulations. In particular, we find that simultaneous minimization of prediction error and conditional expected prediction loss is better than separate minimization of the prediction error or the conditional expected prediction loss.},
author = {M{\"{u}}ller, Samuel and Welsh, Alan H.},
doi = {10.1198/016214505000000529},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bootstrap model selection,Outlier,Robust model selection,Schwarz Bayesian information criterion,Stratified bootstrap},
month = {dec},
number = {472},
pages = {1297--1310},
publisher = {Taylor {\&} Francis},
title = {{Outlier robust model selection in linear regression}},
volume = {100},
year = {2005}
}
@article{Muller2013,
abstract = {Linear mixed effects models are highly flexible in handling a broad range of data types and are therefore widely used in applica-tions. A key part in the analysis of data is model selection, which often aims to choose a parsimonious model with other desirable properties from a possibly very large set of candidate statistical models. Over the last 5–10 years the literature on model selection in linear mixed models has grown extremely rapidly. The problem is much more complicated than in linear regression because selection on the covariance structure is not straightforward due to computational issues and boundary prob-lems arising from positive semidefinite constraints on covariance matri-ces. To obtain a better understanding of the available methods, their properties and the relationships between them, we review a large body of literature on linear mixed model selection. We arrange, implement, discuss and compare model selection methods based on four major ap-proaches: information criteria such as AIC or BIC, shrinkage methods based on penalized loss functions such as LASSO, the Fence procedure and Bayesian techniques.},
author = {M{\"{u}}ller, Samuel and Scealy, J. L. and Welsh, A. H.},
doi = {10.1214/12-sts410},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Statistical Science/M{\"{u}}ller, Scealy, Welsh - 2013 - Model selection in linear mixed models(2).pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {AIC, Bayes factor, BIC, Cholesky decomposition, fe,BIC,Bayes factor,Cholesky decomposition,aic,and phrases,bayes factor,bic,cholesky decomposition,fe,fence,information criteria,lasso,linear mixed model,model selection},
number = {2},
pages = {135--167},
title = {{Model selection in linear mixed models}},
volume = {28},
year = {2013}
}
@article{Mallows1973,
author = {Mallows, C L},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Technometrics/Mallows - 1973 - Some Comments on Cp.pdf:pdf},
journal = {Technometrics},
keywords = {Linear,Regression,Ridge,Selection of,Variables},
number = {4},
pages = {661--675},
title = {{Some Comments on Cp}},
volume = {15},
year = {1973}
}
@article{Clyde2011,
abstract = {For the problem of model choice in linear regression, we introduce$\backslash$na Bayesian adaptive sampling algorithm (BAS), that samples models$\backslash$nwithout replacement from the space of models. For problems that permit$\backslash$nenumeration of all models BAS is guaranteed to enumerate the model$\backslash$nspace in 2p iterations where p is the number of potential variables$\backslash$nunder consideration. For larger problems where sampling is required,$\backslash$nwe provide conditions under which BAS provides perfect samples without$\backslash$nreplacement. When the sampling probabilities in the algorithm are$\backslash$nthe marginal variable inclusion probabilities, BAS may be viewed$\backslash$nas sampling models “near” the median probability model of Barbieri$\backslash$nand Berger. As marginal inclusion probabilities are not known in$\backslash$nadvance we discuss several strategies to estimate adaptively the$\backslash$nmarginal inclusion probabilities within BAS. We illustrate the performance$\backslash$nof the algorithm using simulated and real data and show that BAS$\backslash$ncan outperform Markov chain Monte Carlo methods. The algorithm is$\backslash$nimplemented in the R package BAS available at CRAN.},
author = {Clyde, Merlise A. and Ghosh, Joyee and Littman, Michael L.},
doi = {10.1198/jcgs.2010.09049},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of Computational and Graphical Statistics/Clyde, Ghosh, Littman - 2011 - Bayesian adaptive sampling for variable selection and model averaging.pdf:pdf},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Bayesian model averaging,Inclusion probability,Markov chain Monte Carlo,Median probability model,Model uncertainty,Sampling without replacement},
number = {1},
pages = {80--101},
title = {{Bayesian adaptive sampling for variable selection and model averaging}},
volume = {20},
year = {2011}
}
@book{Venables2002,
abstract = {... Long 1997; Venables and Ripley 1997), and negative binomial (Long 1997; Venables and Ripley 1997). All three of these regression models are parametric ...},
author = {Venables, W. N. and Ripley, B. D.},
doi = {10.1007/978-0-387-21706-2},
isbn = {978-1-4419-3008-8},
publisher = {Springer, New York},
title = {{Modern Applied Statistics with S}},
year = {2002}
}
@book{Mccullagh1989,
author = {McCullagh, P. and Nelder, J. A.},
isbn = {9780412317606},
pages = {526},
pmid = {1855},
publisher = {Chapman {\&} Hall / CRC, New York},
title = {{Generalized Linear Models}},
year = {1989}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
eprint = {arXiv:1011.1669v3},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Annals of Statistics/Schwarz - 1978 - Estimating the dimension of a model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the dimension of a model}},
volume = {6},
year = {1978}
}
@article{Firth1993,
author = {Firth, David},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Biometrics/Firth - 1993 - Bias reduction of maximum likelihood estimates.pdf:pdf},
journal = {Biometrics},
number = {1},
pages = {27--38},
title = {{Bias reduction of maximum likelihood estimates}},
volume = {80},
year = {1993}
}
@article{Bertsimas2016,
abstract = {In the period 1991–2015, algorithmic advances in Mixed Integer Optimization (MIO) coupled with hardware improvements have resulted in an astonishing 450 billion factor speedup in solving MIO problems. We present a MIO approach for solving the classical best subset selection problem of choosing kk out of pp features in linear regression given nn observations. We develop a discrete extension of modern first-order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with nn in the 1000s and pp in the 100s in minutes to provable optimality, and finds near optimal solutions for nn in the 100s and pp in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than Lasso and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.03133v1},
author = {Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
doi = {10.1214/15-AOS1388},
eprint = {arXiv:1507.03133v1},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Annals of Statistics/Bertsimas, King, Mazumder - 2016 - Best subset selection via a modern optimization lens.pdf:pdf},
isbn = {0001415123},
issn = {00905364},
journal = {Annals of Statistics},
number = {2},
pages = {813--852},
title = {{Best subset selection via a modern optimization lens}},
volume = {44},
year = {2016}
}
@article{Furnival1974,
author = {Furnival, G and Wilson, R},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Technometrics/Furnival, Wilson - 1974 - Regressions by leaps and bounds.pdf:pdf},
journal = {Technometrics},
number = {4},
pages = {499--511},
title = {{Regressions by leaps and bounds}},
volume = {16},
year = {1974}
}
@article{Muller2010,
abstract = {P{\textgreater}Many popular methods of model selection involve minimizing a penalized function of the data (such as the maximized log-likelihood or the residual sum of squares) over a set of models. The penalty in the criterion function is controlled by a penalty multiplier lambda which determines the properties of the procedure. In this paper, we first review model selection criteria of the simple form "Loss + Penalty" and then propose studying such model selection criteria as functions of the penalty multiplier. This approach can be interpreted as exploring the stability of model selection criteria through what we call model selection curves. It leads to new insights into model selection and new proposals on how to select models. We use the bootstrap to enhance the basic model selection curve and develop convenient numerical and graphical summaries of the results. The methodology is illustrated on two data sets and supported by a small simulation. We show that the new methodology can outperform methods such as AIC and BIC which correspond to single points on a model selection curve.Resume Beaucoup de methodes populaires de selection de variables impliquent la minimisation d'une fonction penalisee des donnees (comme la vraisemblance maximisee ou la somme residuelle carres) sur un jeu de modeles. La penalite dans la fonction de critere est controlee par un multiplicateur de penalite lambda qui determine les proprietes de la procedure. Nous reconsiderons d'abord des criteres de selection modeles de la forme simple 'Perte + Penalite' et proposons ensuite d'etudier de telles fonctions comme les fonctions du multiplicateur de penalite. Cette approche peut etre interpretee comme l'exploration de la stabilite de fonctions de critere par ce que nous appelons des courbes de choix modeles. Il mene a de nouvelles comprehensions dans le selection de variables et de nouvelles propositions de la facon d'utiliser ces fonctions de critere pour selectionner de variables. Nous utilisons le bootstrap pour augmenter des courbes de choix modele et developpent les resumes numeriques et graphiques des resultats. La methodologie est illustree sur deux jeux de donnees et soutenue par une petite simulation. Nous montrons que la nouvelle methodologie peut surpasser des methodes comme AIC et BIC qui correspond aux points simples sur une courbe de choix modele.},
author = {M{\"{u}}ller, Samuel and Welsh, Alan H.},
doi = {10.1111/j.1751-5823.2010.00108.x},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/International Statistical Review/M{\"{u}}ller, Welsh - 2010 - On model selection curves.pdf:pdf},
isbn = {0306-7734},
issn = {03067734},
journal = {International Statistical Review},
keywords = {Akaike information criterion (AIC),Bayesian information criterion (BIC),Generalized information criterion (GIC),Linear regression,Model selection,Model selection curves},
number = {2},
pages = {240--256},
title = {{On model selection curves}},
volume = {78},
year = {2010}
}
@article{Shao1997,
abstract = {In the problem of selecting a linear model to approximate the true un- known regression model, some necessary and/or sufficient conditions are estab- lished for the asymptotic validity of various model selection procedures such as Akaike's AIC, Mallows' Cp, Shibata's FPE$\lambda$, Schwarz' BIC, generalized AIC, cross- validation, and generalized cross-validation. It is found that these selection proce- dures can be classified into three classes according to their asymptotic behavior. Under some fairly weak conditions, the selection procedures in one class are asymp- totically valid if there exist fixed-dimension correct models; the selection procedures in another class are asymptotically valid if no fixed-dimension correct model exists. The procedures in the third class are compromises of the procedures in the first two classes. Some empirical results are also presented.},
author = {Shao, Jun},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Statistica Sinica/Shao - 1997 - An asymptotic theory for linear model selection.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {aic,and phrases,asymptotic loss efficiency,bic,c p,consistency,cross-,gic,squared error loss,validation},
pages = {221--264},
title = {{An asymptotic theory for linear model selection}},
volume = {7},
year = {1997}
}
@article{Meinshausen2010,
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
file = {:Users/kevinwang/Dropbox (Sydney Uni)/Mendeley/Journal of Royal Statistical Society Series B/Meinshausen, B{\"{u}}hlmann - 2010 - Stability selection.pdf:pdf},
journal = {Journal of Royal Statistical Society Series B},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
number = {4},
pages = {417--473},
title = {{Stability selection}},
volume = {72},
year = {2010}
}
